{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecea567",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "データセットを結合して、学習します。\n",
    "\n",
    "Jetson Orin Nanoでは、2000枚のデータセットでも1時間程度で学習はおわります。Jetson Nanoでは、2000枚のデータセットの学習には10時間程度かかります。Jetson Nanoユーザは、300枚を超える学習時は、下記URLから起動できるColabを試してください。\n",
    "\n",
    "https://colab.research.google.com/drive/1GbDrNiosTKSJNOJiCiVgv6V8X-0GDBfW?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa67f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Jetson Orin Nanoを認識: I2Cバス番号: 7, Powerモード: MODE_7W(1)に設定します。\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Jetson.GPIO as GPIO\n",
    "\n",
    "BOARD_NAME = GPIO.gpio_pin_data.get_data()[0]\n",
    "\n",
    "mode_descriptions = {\n",
    "    \"JETSON_NX\": [\"15W_2CORE\", \"15W_4CORE\", \"15W_6CORE\", \"10W_2CORE\", \"10W_4CORE\"],\n",
    "    \"JETSON_XAVIER\": [\"MAXN\", \"MODE_10W\", \"MODE_15W\", \"MODE_30W\"],\n",
    "    \"JETSON_NANO\": [\"MAXN\", \"5W\"],\n",
    "    \"JETSON_ORIN\": [\"MAXN\", \"MODE_15W\", \"MODE_30W\", \"MODE_40W\"],\n",
    "    \"JETSON_ORIN_NANO\": [\"MODE_15W\", \"MODE_7W\"]\n",
    "}\n",
    "\n",
    "product_names = {\n",
    "    \"JETSON_NX\": \"Jetson Xavier NX\",\n",
    "    \"JETSON_XAVIER\": \"Jetson AGX Xavier\",\n",
    "    \"JETSON_NANO\": \"Jetson Nano\",\n",
    "    \"JETSON_ORIN\": \"Jetson AGX Orin\",\n",
    "    \"JETSON_ORIN_NANO\": \"Jetson Orin Nano\"\n",
    "}\n",
    "\n",
    "# ボードごとのI2Cバス番号と初期Powerモードを定義する\n",
    "board_settings = {\n",
    "    \"JETSON_NX\": (8, 3),\n",
    "    \"JETSON_XAVIER\": (8, 2),\n",
    "    \"JETSON_NANO\": (1, 0),\n",
    "    \"JETSON_ORIN\": (7, 0),\n",
    "    \"JETSON_ORIN_NANO\": (7, 1)\n",
    "}\n",
    "\n",
    "i2c_busnum, power_mode = board_settings.get(BOARD_NAME, (None, None))\n",
    "mode_description = mode_descriptions.get(BOARD_NAME, [])\n",
    "product_name = product_names.get(BOARD_NAME, \"未知のボード\")\n",
    "\n",
    "if power_mode is not None and power_mode < len(mode_description):\n",
    "    mode_str = mode_description[power_mode]\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"{product_name}を認識: I2Cバス番号: {i2c_busnum}, Powerモード: {mode_str}({power_mode})に設定します。\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "else:\n",
    "    print(\"未知のボードまたは不正なモードです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0eeaffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker起動のため電力モードは変更できません。\n"
     ]
    }
   ],
   "source": [
    "if (product_name == \"Jetson Orin Nano\") or (product_name == \"Jetson AGX Orin\"):\n",
    "    print(\"Docker起動のため電力モードは変更できません。\")\n",
    "else:\n",
    "    !echo \"jetson\" | sudo -S nvpmodel -m $power_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13adbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVPM WARN: power mode is not set!\n"
     ]
    }
   ],
   "source": [
    "!echo \"jetson\" | sudo -S nvpmodel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299da607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker起動のためjetson_clocksは起動できません。\n"
     ]
    }
   ],
   "source": [
    "if (product_name == \"Jetson Orin Nano\") or (product_name == \"Jetson AGX Orin\"):\n",
    "    print(\"Docker起動のためjetson_clocksは起動できません。\")\n",
    "else:\n",
    "    !echo \"jetson\" | sudo -S jetson_clocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5e43a-34f0-47d6-9927-3fbf860b9d2c",
   "metadata": {},
   "source": [
    "## Datasetを指定\n",
    "\n",
    "DATA_SETSの配列は、自分の作成したデータ設定名に修正します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3db388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SETS = [\"dataset/aizu_set_001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fcbd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "from xy_dataset import XYDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_data(path=''):\n",
    "    global dataset\n",
    "    CATEGORIES = ['xy','speed']\n",
    "    TRANSFORMS = transforms.Compose([\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = XYDataset(path, CATEGORIES, TRANSFORMS, random_hflip=True)\n",
    "    print(f'データを{len(dataset)} 件読み込みました')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577ffc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データを0 件読み込みました\n",
      "全データセットを結合しました。合計 0 件のデータがあります。\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "all_datasets = []\n",
    "for dataset_path in DATA_SETS:\n",
    "    dataset = load_data(dataset_path)\n",
    "    all_datasets.append(dataset)\n",
    "\n",
    "# Concatenate all datasets\n",
    "full_dataset = ConcatDataset(all_datasets)\n",
    "print(f'全データセットを結合しました。合計 {len(full_dataset)} 件のデータがあります。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769b63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def pretrained_model():\n",
    "    # ALEXNET\n",
    "    # model = torchvision.models.alexnet(pretrained=True)\n",
    "    # model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "    # SQUEEZENET\n",
    "    # model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "    # model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "    # model.num_classes = len(dataset.categories)\n",
    "\n",
    "    # RESNET 18\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "    # RESNET 34\n",
    "    # model = torchvision.models.resnet34(pretrained=True)\n",
    "    # model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "    # DENSENET 121\n",
    "    # model = torchvision.models.densenet121(pretrained=True)\n",
    "    # model.classifier = torch.nn.Linear(model.classifier.in_features, output_dim)\n",
    "\n",
    "    return model\n",
    "\n",
    "def weights_model():\n",
    "    # ALEXNET\n",
    "    # model = torchvision.models.alexnet(weights=torchvision.models.AlexNet_Weights.DEFAULT)\n",
    "    # model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "    # SQUEEZENET\n",
    "    # model = torchvision.models.squeezenet1_1(weights=torchvision.models.SqueezeNet1_1_Weights.DEFAULT)\n",
    "    # model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "    # model.num_classes = len(dataset.categories)\n",
    "\n",
    "    # RESNET 18\n",
    "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "    model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "    # RESNET 34\n",
    "    # model = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n",
    "    # model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "    # DENSENET 121\n",
    "    # model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.DEFAULT)\n",
    "    # model.classifier = torch.nn.Linear(model.classifier.in_features, output_dim)\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_pretrained_model():\n",
    "    global model\n",
    "    print('Pre-trainedモデルを読み込みます。')\n",
    "    # torchvisionのバージョン文字列を取得\n",
    "    version_str = torchvision.__version__\n",
    "\n",
    "    # 正規表現でメジャー、マイナー、パッチのバージョンを抜き出す\n",
    "    match = re.match(r'(\\d+)\\.(\\d+)\\.(\\d+)', version_str)\n",
    "    if match:\n",
    "        major, minor, _ = map(int, match.groups())\n",
    "        # 0.13以上の場合\n",
    "        if major > 0 or minor >= 13:\n",
    "            # pretrainedが非推奨となったため、最新の学習済みwightsを使う\n",
    "            # https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/\n",
    "            model = weights_model()\n",
    "        else:\n",
    "            # pretrainedを使う\n",
    "            model = pretrained_model()\n",
    "    else:\n",
    "        print(\"Unable to parse torchvision version\")\n",
    "\n",
    "def load_model(model_file):\n",
    "    global model, optimizer, output_dim\n",
    "    # 前提：datasetを読み込み済み\n",
    "    output_dim = 2 * len(dataset.categories)  # x, y coordinate for each category\n",
    "\n",
    "    # モデルを読み込みます\n",
    "    load_pretrained_model()\n",
    "\n",
    "    # 学習済みの重みがあれば読み込みます\n",
    "    if os.path.exists(model_file):\n",
    "        print(f'重み情報{model_file}を読み込みます。')\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def save_model(model_file):\n",
    "    # 学習済みの重みを.pthファイルに保存します。(モデル構造は含みません)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    print(\"学習結果を\" + model_file + \"に保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e14cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "038ac6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期の最良の損失値を無限大として設定\n",
    "best_loss = float('inf')\n",
    "# 学習と評価の損失の履歴\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "# エポックの履歴\n",
    "epochs = []\n",
    "\n",
    "def filter_none(data):\n",
    "    return [(images, category_idx, xy) for images, category_idx, xy in data if images is not None and xy is not None and category_idx is not None]\n",
    "\n",
    "def train_eval(is_training=True, batch_size=8, epoch=20, stop_count=10):\n",
    "    global model, full_dataset, optimizer, best_loss\n",
    "\n",
    "        # データセットを学習用とテスト用に分割\n",
    "    valid_data = []\n",
    "    for i in range(len(full_dataset)):\n",
    "        try:\n",
    "            _ = full_dataset[i]\n",
    "            valid_data.append(full_dataset[i])\n",
    "        except AttributeError as e:\n",
    "            print(f\"無効なデータが検出されました（インデックス：{i}）: {e}\")\n",
    "\n",
    "    full_dataset = valid_data\n",
    "\n",
    "    full_dataset = filter_none(full_dataset)  # Noneデータを除外\n",
    "    total_size = len(full_dataset)\n",
    "    total_size = len(full_dataset)\n",
    "    split = total_size * 10 // 100  # １0%をテストデータとして使用\n",
    "    indices = list(range(total_size))\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_dataset = data.Subset(full_dataset, train_indices)\n",
    "    test_dataset = data.Subset(full_dataset, test_indices)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.train()\n",
    "    non_improving_epochs = 0\n",
    "    epoch_count = 0\n",
    "\n",
    "    try:\n",
    "        while epoch > 0:\n",
    "            sum_train_loss = 0.0\n",
    "\n",
    "            # Plot\n",
    "            data_size = len(train_loader) * batch_size\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(epochs, train_losses, label='Train Loss')\n",
    "            plt.plot(epochs, test_losses, label='Test Loss')\n",
    "            title = 'Train Loss vs. Test Loss (' + str(data_size) +' datas)'\n",
    "            plt.title(title)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "            # 時刻計測\n",
    "            start_time = time.time()\n",
    "\n",
    "            #scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "            # 学習の進行状況を表示するプログレスバー\n",
    "            progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch_count + 1}\")\n",
    "\n",
    "            for i, (images, category_idx, xy) in progress_bar:\n",
    "                if images is None or xy is None:\n",
    "                    print(\"Warning: None type data found at index\", i)\n",
    "                    continue\n",
    "                images = images.to(device)\n",
    "                xy = xy.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = 0.0\n",
    "                for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                loss /= len(category_idx)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                sum_train_loss += float(loss)\n",
    "\n",
    "                # 進行状況バーに損失値を表示\n",
    "                average_loss = sum_train_loss / (i + 1)\n",
    "                progress_bar.set_description(f\"Epoch {epoch_count + 1} Loss: {average_loss:.5f}\")\n",
    "\n",
    "            train_loss = sum_train_loss / len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Evaluate\n",
    "            model = model.eval()\n",
    "            sum_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, category_idx, xy in test_loader:\n",
    "                    if images is None or xy is None:\n",
    "                      print(\"Error: None type data found at index\", i)\n",
    "                      continue\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    xy = xy.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = 0.0\n",
    "                    for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                        loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                    loss /= len(category_idx)\n",
    "                    sum_test_loss += float(loss)\n",
    "\n",
    "            test_loss = sum_test_loss / len(test_loader)\n",
    "            test_losses.append(test_loss)\n",
    "            epoch_count += 1\n",
    "            epochs.append(epoch_count)\n",
    "\n",
    "            best_model = False\n",
    "            # Early stopping check\n",
    "            if test_loss < best_loss:   # Check if the current test_loss is the best\n",
    "                best_loss = test_loss   # Update best_loss with test_loss\n",
    "                non_improving_epochs = 0\n",
    "                model_dir = \"./model\"\n",
    "                if not os.path.exists(model_dir):\n",
    "                    os.makedirs(model_dir)\n",
    "                save_model(model_dir + \"/best_model.pth\")\n",
    "                print(\"Saved best model with test loss:\", best_loss)\n",
    "                best_model = True\n",
    "            else:\n",
    "                non_improving_epochs += 1\n",
    "\n",
    "            # スケジューラの更新\n",
    "            # scheduler.step(test_loss)  # 注意: ここには評価データの損失を入れます\n",
    "\n",
    "            epoch -= 1\n",
    "            model = model.train()\n",
    "\n",
    "            end_time = time.time()  # stop measuring time\n",
    "            epoch_duration = end_time - start_time  # calculate the duration for the epoch\n",
    "            total_time = (epoch_count + 1) * epoch_duration  # calculate total time taken for all epochs\n",
    "\n",
    "            # ファイルに追記\n",
    "            with open(\"./log.txt\", \"a\") as file:\n",
    "                file.write(f\"Epoch {epoch_count}: Train Loss: {loss:.5f}, Test Loss: {test_loss:.5f}, Best Model: {best_model}, Time: {epoch_duration/60:.4f} 分, Total time: {total_time/60:.4f} 分\\n\")\n",
    "\n",
    "            if non_improving_epochs >= stop_count:\n",
    "                print(\"Loss hasn't improved for {} consecutive epochs. Stopping training.\".format(MAX_NON_IMPROVING_EPOCHS))\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd1f132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trainedモデルを読み込みます。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ed1adddf3f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# best_model.pthを追加で学習する場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#load_model(\"best_model.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NON_IMPROVING_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-22d1eaace644>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(is_training, batch_size, epoch, stop_count)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/virtualenv/python3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/virtualenv/python3/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "MAX_NON_IMPROVING_EPOCHS = 30\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "# 初期の最良の損失値を無限大として設定\n",
    "best_loss = float('inf')\n",
    "# 学習と評価の損失の履歴\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "# エポックの履歴\n",
    "epochs = []\n",
    "\n",
    "load_model(\"\")\n",
    "# best_model.pthを追加で学習する場合\n",
    "#load_model(\"best_model.pth\")\n",
    "train_eval(batch_size=BATCH_SIZE, epoch=EPOCHS, stop_count=MAX_NON_IMPROVING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97f143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d28e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c483ac-6eaa-4227-add7-5a838c01ce26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
