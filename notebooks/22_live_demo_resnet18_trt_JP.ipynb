{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance - ResNet18 TensorRTでの自動走行\n",
    "\n",
    "Pytorchで学習したモデルをTensorRTモデルに変換したことで高速処理が可能になりました。  \n",
    "このノートブックでは、TensorRT化したモデルを使うことでカクツキを抑えてJetBotがなめらかに走行することを確認できます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# デバイスの準備\n",
    "\n",
    "カメラ画像をGPUメモリに転送するために、先に定義だけしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torch\n",
    "\n",
    "########################################\n",
    "# TensorRTの場合、モデルはGPUを使うように実装されていますが、\n",
    "# 入力データはGPUメモリに転送する必要があります。\n",
    "# そのための定義をここでしておきます。\n",
    "########################################\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRTに最適化されたモデル``best_model_trt.pth``を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torchvision  # これはすでに読込んでいるため、省略可能です。\n",
    "from torch2trt import TRTModule  # TensorRTのライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# TensorRTモデルを読み込みます。\n",
    "########################################\n",
    "model_trt = TRTModule()  # TensorRTモデルを読み込むための変数を定義します。\n",
    "model_trt.load_state_dict(torch.load('detection_trt.pth'))  # 学習したTensorRTモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カメラ画像の前処理作成\n",
    "モデルを読み込みましたが、まだ少し問題があります。  \n",
    "学習時の入力画像フォーマットと、OpenCVのカメラ画像フォーマットは一致しません。  \n",
    "これを解消するために、 いくつかの前処理を行う必要があります。これらは、下記の手順になります。\n",
    "\n",
    "1. カメラ画像をBGRフォーマットからRGBフォーマットに変換します。\n",
    "> 学習時のjpeg画像はtorchvision.datasets.ImageFolderによって読み込まれます。ImageFolderはPILライブラリを利用して画像ファイルを読み込んだ後、RGBフォーマットに変換しています。このため、学習時の入力画像データはRGBフォーマットになっています。しかしカメラ画像を取得するために使っているOpenCVはデフォルトでBGRフォーマットとなるため、このまま予測すると画像の赤色と青色が入れ替わっているために精度が悪くなります。そこで、カメラ画像を学習時のフォーマットに合わせるためにRGBフォーマットに変換します。\n",
    "2. HWCをCHWに変換します。\n",
    "> cudnnはHWC(Height x Width x Channel)をサポートしません。そのため画像情報の並び順をHWCからCHW(Channel x Height x Width)に変換します。\n",
    "3. カメラ画像を正規化します。\n",
    "> トレーニング中に使ったのと同じパラメータ（平均と標準偏差）を利用してカメラ画像の各チャンネル(RGB)を正規化します。  \n",
    "> OpenCVで取得したカメラ画像の1ピクセルはRGBをそれぞれint型で[0, 255]の範囲で表したものになります。  \n",
    "> しかし、学習時はImageFolderを使ってjpeg画像を読込み、それをtransforms.ToTensor()を使ってTensor型に変換しています。この時、CHWへの変換と計算グラフレイヤの追加の他に、RGB値がint型の[0, 255]からfloat型の[0.0, 1.0]にスケーリングされています。学習時はこの[0.0, 1.0]の値に対してtransforms.Normalize()を行うことでRGB各値を正規化（ImageNetデータセットのRGB毎に平均を0、標準偏差が1になるようにスケーリング）しています。 \n",
    "> ここではToTensor()を使わずにCHWへの変換をおこなっています。そしてImageNetと同じ範囲に各チャンネルをスケーリングするために、Normalize()に渡すパラメータに255.0を掛けています。\n",
    "4. カメラ画像をGPUメモリに転送します。\n",
    "> 入力データはモデルと同じデバイスに存在する必要があります。\n",
    "5. 入力画像データを配列に変更します。\n",
    "> 学習時はバッチサイズ分の画像を配列にして入力層に与えています。モデルの入力層は学習ために入力データを可変長の配列で受け取る構造になっています。そのため予測時は1枚の画像であっても入力画像データを配列にする必要があります。\n",
    "\n",
    "* ImageFolderリファレンス：\n",
    "  * https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n",
    "* ImageFolder実装コード：\n",
    "  * https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "* Normalizeリファレンス：\n",
    "  * https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "* Normalize実装コード：\n",
    "  * https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py\n",
    "* 正規化パラメータの値の理由：\n",
    "  * https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
    "* 正規化に意味があるのかどうか：\n",
    "  * https://teratail.com/questions/234027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "########################################\n",
    "# 衝突回避モデルのプリプロセッシング処理の中で使われます。\n",
    "# この値はpytorch ImageNetの学習に使われた正規化のパラメータです。（ImageNetデータセットのRGB毎に平均を0、標準偏差が1になるようにスケーリングすること）\n",
    "# カメラ画像はこの値でRGBを正規化することが望ましいでしょう。\n",
    "########################################\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "########################################\n",
    "# この正規化の定義は使っていません。\n",
    "# 代わりに\n",
    "# image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "# で正規化しています。\n",
    "########################################\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "########################################\n",
    "# カメラ画像をモデル入力用データに変換します。\n",
    "########################################\n",
    "def preprocess(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # カメラ映像はOpenCVで読み込んでいるため画像はBGRフォーマットになっています。これをRGBフォーマットに変換します。\n",
    "    image = PIL.Image.fromarray(image)  # OpenCV画像データ(配列データ）をPILイメージオブジェクトに変換します。\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()  # 画像をTensor型に変換してfloat16型でGPUメモリに転送します。\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])  # ImageNetのパラメータで正規化します。\n",
    "    return image[None, ...]  # バッチ配列に変換して返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> すばらしい、これでカメラ画像をニューラルネットワークの入力フォーマットに変換するための、pre-processing関数を定義できました。　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カメラの動作確認\n",
    "それでは、カメラを起動して表示しましょう。  \n",
    "JetBotが「blocked」（旋回する）と判断している確率を表示するためのスライダーも用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import traitlets  # カメラ画像などのデータが更新されたときに、連動して処理を実行させるためにtraitletsライブラリを利用します。\n",
    "from IPython.display import display  # ウィジェットを表示するためのdisplayライブラリを利用します。\n",
    "import ipywidgets.widgets as widgets  # Jupyter標準のウィジェットを利用します。\n",
    "from jetbot import Camera, bgr8_to_jpeg  # JetBot用に用意したカメラと画像変換ライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# カメラを有効化します。\n",
    "# 画像はwidthとheightで指定したピクセルサイズにリサイズされます。\n",
    "# fpsのデフォルトは30ですが、カメラフレーム更新に連動して推論を実行するようにコーディングしているため、\n",
    "# 処理が重くなってしまいます。そのためfpsを小さく設定します。\n",
    "########################################\n",
    "camera = Camera.instance(width=224, height=224, fps=4)\n",
    "\n",
    "########################################\n",
    "# 画像表示用のウィジェットを用意します。\n",
    "# widthとheightは表示するウィジェットの幅と高さです。\n",
    "# カメラ画像サイズと一致する必要はありません。\n",
    "########################################\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "########################################\n",
    "# 「blocked」の確率を表示するためのスライダーを用意します。\n",
    "########################################\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "########################################\n",
    "# 「speed」を調整するためのスライダーを用意します。\n",
    "########################################\n",
    "speed_slider = widgets.FloatSlider(description='speed', min=0.0, max=0.5, value=0.0, step=0.01, orientation='horizontal')\n",
    "\n",
    "########################################\n",
    "# traitletsライブラリを利用してカメラ画像データが更新されたときに、\n",
    "# bgr8フォーマットをjpegフォーマットに変換してから\n",
    "# 画像表示ウィジェットに反映するように設定します。\n",
    "########################################\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "########################################\n",
    "# 画像表示ウィジェットとスライダーをブラウザに表示します。\n",
    "########################################\n",
    "display(widgets.VBox([widgets.HBox([image, blocked_slider]), speed_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モーターを制御するためにrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import Robot  # JetBotを制御するためのライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# JetBotの制御用クラスをインスタンス化します。\n",
    "########################################\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、カメラの画像が更新されるたびに呼び出される関数を生成します。この関数は、下記ステップを実行します。\n",
    "\n",
    "1. カメラ画像をPre-processingにかけてモデル入力データに変換します。\n",
    "2. モデルの推論を実行します。\n",
    "3. 推論結果が50%以上の確率で「blocked」の場合は、左に曲がります。それ以外の場合は前進します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "########################################\n",
    "# カメラ画像が更新されたときに実行する処理を定義します。\n",
    "########################################\n",
    "def update(change):\n",
    "    ########################################\n",
    "    # この関数内で参照だけされる定義は暗黙的にグローバル定義となります。\n",
    "    # しかし、この関数内で値を代入される定義は暗黙的にローカル定義となります。\n",
    "    # blocked_sliderとrobotはそれ自体に値が代入されていないため、暗黙的にグローバル定義となります。\n",
    "    # そのためここでのglobal宣言は省略可能です。\n",
    "    ########################################\n",
    "    global blocked_slider, robot\n",
    "    # カメラ画像を変数xにコピーします。\n",
    "    x = change['new'] \n",
    "    # カメラ画像をモデルの入力データに変換します。\n",
    "    x = preprocess(x)\n",
    "    # 推論を実行します。\n",
    "    y = model_trt(x)\n",
    "    #print(y)\n",
    "    \n",
    "    # softmax()関数を適用して出力ベクトルの合計が1になるように正規化します（これにより確率分布になります）\n",
    "    y = F.softmax(y, dim=1)\n",
    "    #print(y)\n",
    "    \n",
    "    # 入力データは多次元のバッチ配列になっています。出力もそれに対応しているためyは多次元配列になっています。\n",
    "    # y.flatten()を呼び出すことで可能な限り不要な次元を除去します。([[blocked_rate, free_rate]]を[blocked_rate, free_rate]に変換)\n",
    "    # そのうえで、「blocked」の確率となるy.flatten()[0]の値を取得します。「free」の確率を取得する場合はy.flatten()[1]になります。\n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    #print(prob_blocked)\n",
    "    \n",
    "    # 「blocked」の確率をスライダーに反映します。\n",
    "    blocked_slider.value = prob_blocked\n",
    "    \n",
    "    # 「blocked」の確率が50%未満なら直進します。それ以外は左に旋回します。\n",
    "    if prob_blocked < 0.5:\n",
    "        robot.forward(speed_slider.value)  # JetBotのモーター出力をspeedスライダーの値にして前進します。\n",
    "    else:\n",
    "        robot.left(speed_slider.value)  # JetBotのモーター出力をspeedスライダーの値にして左に旋回します。\n",
    "    \n",
    "    time.sleep(0.001)  # 値がモーター制御基板のICチップに反映されるまで少し待ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル推論からJetBotの動作までを実行する関数を作成しました。  \n",
    "今度はそれをカメラ画像の更新に連動して動作させる必要があります。\n",
    "\n",
    "JetBotでは、traitlets.HasTraitsを継承したCameraクラスを実装しているので、observe()を呼び出すだけで実現できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JetBotを動かしてみよう\n",
    "次のコードで``start jetbot``ボタンと``stop jetbot``ボタンを作成します。  \n",
    "\n",
    "``start jetbot``ボタンを押すとモデルの初期化が実行されます。  \n",
    "モデルの初期化が完了すると、``blocked``スライダーが動作し始めます。  \n",
    "``speed``スライダーを動かすとJetBotが動作し始めます。  \n",
    "\n",
    "``stop jetbot``ボタンを押すとJetBotが停止します。  \n",
    "最初の1フレームの実行時にメモリの初期化が実行されるので、ディープラーニングではどんなモデルも最初の1フレームの処理はすこし時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# スタートボタンとストップボタンを作成します。\n",
    "########################################\n",
    "model_start_button = widgets.Button(description='start jetbot')\n",
    "model_stop_button = widgets.Button(description='stop jetbot')\n",
    "\n",
    "########################################\n",
    "# スタートボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################\n",
    "def start_model(c):\n",
    "    update({'new': camera.value})  # update()関数を1回呼び出して初期化します。\n",
    "    camera.observe(update, names='value')  # Cameraクラスのtraitlets.Any()型のvalue変数(カメラ画像データ)が更新されたときに指定した関数を呼び出します。\n",
    "model_start_button.on_click(start_model)  # startボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ストップボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################    \n",
    "def stop_model(c):\n",
    "    camera.unobserve(update, names='value')  # カメラ画像が更新されたときにupdate()関数を呼び出していた接続を解除します。\n",
    "    time.sleep(1)  # フレームの処理の完了を待つためのスリープを追加します。\n",
    "    robot.stop()  # モーターを停止します。\n",
    "model_stop_button.on_click(stop_model)  # stopボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ウィジェットの表示レイアウトを定義します。\n",
    "########################################\n",
    "model_widget = widgets.VBox([\n",
    "    widgets.VBox([widgets.HBox([image, blocked_slider]), speed_slider]),\n",
    "    widgets.HBox([model_start_button, model_stop_button])\n",
    "])\n",
    "\n",
    "########################################\n",
    "# ウィジェットを表示します。\n",
    "########################################\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カメラの停止\n",
    "最後に、他のノートブックでカメラを使うために、このノートブックで使ったカメラを停止しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.unlink()  # ブラウザへのストリーミングを停止します。（JetBot本体でのカメラは動作し続けます。）\n",
    "camera.stop()  # カメラを停止します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結論\n",
    "このライブデモは以上です。うまくいけばあなたのJetBotは賢く衝突を避けていることでしょう。\n",
    "\n",
    "collision avoidanceが上手く行かない場合、正しく走行できるように失敗しやすい場所でさらにデータを追加してください。  \n",
    "このようにうまくいかない場所を中心にデータを収集すれば、JetBotはさらによく動作するはずです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
